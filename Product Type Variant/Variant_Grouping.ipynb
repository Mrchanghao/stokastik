{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "stdout = sys.stdout\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')\n",
    "sys.stdout = stdout\n",
    "\n",
    "import pandas as pd\n",
    "import common_utils as utils\n",
    "import grouping\n",
    "reload(grouping)\n",
    "from grouping import Grouping\n",
    "import variant_criteria\n",
    "reload(variant_criteria)\n",
    "from variant_criteria import Variant\n",
    "import json\n",
    "\n",
    "product_data = []\n",
    "\n",
    "for df_chunk in pd.read_csv('/Users/a0m02fp/pcf_dump_shoes.tsv', sep='\\\\t', chunksize=10**4, engine='python', encoding='utf-8'):\n",
    "    jsons = list(df_chunk[u'product_json'])\n",
    "    \n",
    "    for item in jsons:\n",
    "        item = json.loads(item)\n",
    "        \n",
    "        if 'abstract_product_id' in item and item['abstract_product_id'] != '':\n",
    "            abstract_product_id = item['abstract_product_id']\n",
    "            attributes = item['product_attributes']\n",
    "\n",
    "            attr_val_pairs = dict()\n",
    "\n",
    "            for attr_key, vals in attributes.items():\n",
    "                value = vals['values'][0]['value']\n",
    "                attr_val_pairs[attr_key] = value\n",
    "\n",
    "            item_id = attributes['item_id']['values'][0]['value'] if 'item_id' in attributes else None\n",
    "            title = attributes['product_name']['values'][0]['value'] if 'product_name' in attributes else None\n",
    "            long_desc = attributes['product_long_description']['values'][0]['value'] if 'product_long_description' in attributes else None\n",
    "            shrt_desc = attributes['product_short_description']['values'][0]['value'] if 'product_short_description' in attributes else None\n",
    "            pt = attributes['product_type']['values'][0]['value'] if 'product_type' in attributes else None\n",
    "            \n",
    "            variant_criterias = dict()\n",
    "            \n",
    "            if 'grouping' in item and 'variant' in item['grouping'] and 'attributes' in item['grouping']['variant']:\n",
    "                variants = item['grouping']['variant']['attributes']\n",
    "                \n",
    "                for attr in variants:\n",
    "                    if 'values' in variants[attr]:\n",
    "                        var_attr = variants[attr]['values'][0]['value']\n",
    "                        variant_criterias[attr] = var_attr\n",
    "        \n",
    "            product_data.append((item_id, pt, title, long_desc, shrt_desc, json.dumps(attr_val_pairs), abstract_product_id, json.dumps(variant_criterias)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(product_data, columns=['item_id', 'product_type', 'title', 'long_desc', 'short_desc', 'attr_val_pairs', 'abstract_product_id', 'variant_criterias'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('items_data_shoes.csv', sep=',', encoding='utf-8', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pts = [x[1] for x in product_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "stdout = sys.stdout\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')\n",
    "sys.stdout = stdout\n",
    "\n",
    "import pandas as pd\n",
    "import common_utils as utils\n",
    "reload(utils)\n",
    "import grouping\n",
    "reload(grouping)\n",
    "from grouping import Grouping\n",
    "import variant_criteria\n",
    "reload(variant_criteria)\n",
    "from variant_criteria import Variant\n",
    "import json\n",
    "\n",
    "import siamese_network\n",
    "reload(siamese_network)\n",
    "\n",
    "import siamese_api\n",
    "reload(siamese_api)\n",
    "\n",
    "import word2vec_api\n",
    "reload(word2vec_api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"Reading data file...\"\n",
    "\n",
    "df = pd.read_csv('items_data.csv', sep=\",\", encoding='utf-8')\n",
    "df['attr_val_pairs'] = df['attr_val_pairs'].apply(lambda x: json.loads(x))\n",
    "df['variant_criterias'] = df['variant_criterias'].apply(lambda x: json.loads(x))\n",
    "\n",
    "print \"Reading product data...\"\n",
    "product_data = list(df.itertuples(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_type, group_type, grouping_algo = \"Bras\", \"auto\", \"hierarchical\"\n",
    "\n",
    "print \"Getting items...\"\n",
    "items = utils.get_unique_items_pt(product_data, item_type)\n",
    "\n",
    "print len(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"Creating groups...\"\n",
    "grp_instance = Grouping(items, similarity_threshold=0.5)\n",
    "grp_instance.init_groups()\n",
    "\n",
    "print grp_instance.get_clustering_scores()\n",
    "\n",
    "groups = grp_instance.auto_groups if group_type == 'auto' else grp_instance.true_groups\n",
    "    \n",
    "print \"Reading excluded attribute list...\"\n",
    "with open('excluded_attr_list.txt', 'rb') as x_attr:\n",
    "    excluded_attrs = x_attr.readlines()\n",
    "\n",
    "excluded_attrs = [x.strip() for x in excluded_attrs] \n",
    "excluded_attrs = set(excluded_attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from siamese_api import SiameseAPI\n",
    "sapi = SiameseAPI(items, grp_instance.true_groups, max_num_tokens=50)\n",
    "sapi.train_model()\n",
    "sapi.insert_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "d = sapi.get_nearest_neighbors_mongo(items[2][2], groups, head_thres=0.5, ind_thres=0.999)\n",
    "print time.time()-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items[2][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in d:\n",
    "    print x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_embeds = sapi.get_representations([str(x[2]) for x in items])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp_instance1 = Grouping(items, representations=s_embeds, similarity_threshold=1.0)\n",
    "grp_instance1.init_groups()\n",
    "groups1 = grp_instance1.auto_groups\n",
    "print grp_instance1.get_clustering_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(groups1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"Fetching important attributes...\"\n",
    "var_instance1 = Variant(groups1, items, max_attrs_per_var=1, max_variants=10, excluded_attrs=excluded_attrs)\n",
    "valid_attrs = set([attr[0] for attr, score in var_instance1.get_variant_scores()])\n",
    "    \n",
    "print \"Fetching variant criteria...\"\n",
    "var_instance = Variant(groups1, items, max_attrs_per_var=2, max_variants=1, excluded_attrs=excluded_attrs, valid_attrs=valid_attrs)\n",
    "variant_scores = var_instance.get_variant_scores()\n",
    "\n",
    "print \"Predominant Variant Criteria : \", variant_scores[0]\n",
    "\n",
    "print \"Fetching variant criteria per group...\"\n",
    "pred_grp_variants = var_instance.get_predicted_variants()\n",
    "\n",
    "print \"Results\"\n",
    "var_instance.results(pred_grp_variants, variant_scores[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from word2vec_api import Word2VecAPI\n",
    "wapi = Word2VecAPI(items)\n",
    "wapi.train_model()\n",
    "wapi.insert_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "d = wapi.get_nearest_neighbors_mongo(items[0][2], groups, head_thres=0.5, ind_thres=0.999)\n",
    "print time.time()-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in d:\n",
    "    print x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_embeds = wapi.get_representations([str(x[2]) for x in items])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp_instance2 = Grouping(items, representations=w_embeds, similarity_threshold=0.1)\n",
    "grp_instance2.init_groups()\n",
    "groups2 = grp_instance2.auto_groups\n",
    "print grp_instance2.get_clustering_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(groups2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"Fetching important attributes...\"\n",
    "var_instance1 = Variant(groups2, items, max_attrs_per_var=1, max_variants=10, excluded_attrs=excluded_attrs)\n",
    "valid_attrs = set([attr[0] for attr, score in var_instance1.get_variant_scores()])\n",
    "    \n",
    "print \"Fetching variant criteria...\"\n",
    "var_instance = Variant(groups2, items, max_attrs_per_var=2, max_variants=1, excluded_attrs=excluded_attrs, valid_attrs=valid_attrs)\n",
    "variant_scores = var_instance.get_variant_scores()\n",
    "\n",
    "print \"Predominant Variant Criteria : \", variant_scores[0]\n",
    "\n",
    "print \"Fetching variant criteria per group...\"\n",
    "pred_grp_variants = var_instance.get_predicted_variants()\n",
    "\n",
    "print \"Results\"\n",
    "var_instance.results(pred_grp_variants, variant_scores[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KDTree\n",
    "import numpy as np\n",
    "tree1 = KDTree(s_embeds, leaf_size=50)\n",
    "tree2 = KDTree(w_embeds, leaf_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = tree1.query_radius(s_embeds[2], r=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = tree2.query_radius(w_embeds[2], r=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in ind[0]:\n",
    "    print items[x][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KDTree\n",
    "import numpy as np\n",
    "tree = KDTree(np.tile(embeds.T, 50).T, leaf_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('tree.pkl', 'wb') as f:\n",
    "    pickle.dump(tree, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, random\n",
    "\n",
    "out = []\n",
    "for j in range(5, 50, 5):\n",
    "    tree = KDTree(np.tile(embeds.T, j).T, leaf_size=50)\n",
    "    w = random.sample(range(len(items)), 1000)\n",
    "    times = []\n",
    "    for i in w:\n",
    "        start = time.time()\n",
    "        ind = tree.query_radius(embeds[i], r=0.02)\n",
    "        duration = time.time()-start\n",
    "        times.append(duration)\n",
    "    out.append((j, np.max(times)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = out + [(50, 0.028570890426635742)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "w = random.sample(range(len(items)), 1000)\n",
    "times = []\n",
    "for i in w:\n",
    "    start = time.time()\n",
    "    ind = tree.query_radius(embeds[i], r=0.02)\n",
    "    duration = time.time()-start\n",
    "    times.append(duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "175*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib.pyplot import plot\n",
    "\n",
    "x, y = zip(*out)\n",
    "plot(x, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
