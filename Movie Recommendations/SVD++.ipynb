{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import os, random\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from scipy import sparse\n",
    "\n",
    "dataset_path = \"/Users/a0m02fp/Downloads/ml-20m\"\n",
    "\n",
    "ratings_df = pd.read_csv(os.path.join(dataset_path, \"ratings.csv\"), encoding=\"utf-8\", sep=\",\")\n",
    "\n",
    "user_id, movie_id, ratings = list(ratings_df[u'userId']), list(ratings_df[u'movieId']), list(ratings_df[u'rating'])\n",
    "\n",
    "uid_mid_pairs = zip(user_id, movie_id, ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid_map = dict()\n",
    "\n",
    "user_ids = sorted(list(set(user_id)))\n",
    "\n",
    "for idx in range(len(user_ids)):\n",
    "    uid_map[user_ids[idx]] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = pd.read_csv(os.path.join(dataset_path, \"movies.csv\"), encoding=\"utf-8\", sep=\",\")\n",
    "\n",
    "movie_ids, mid_titles = list(movies_df[u'movieId']), list(movies_df[u'title'])\n",
    "\n",
    "mid_to_title_map = dict()\n",
    "\n",
    "mid_map, mid_reverse_map = dict(), dict()\n",
    "\n",
    "for mid, title in zip(movie_ids, mid_titles):\n",
    "    mid_to_title_map[mid] = title\n",
    "    \n",
    "for idx in range(len(movie_ids)):\n",
    "    mid_reverse_map[idx] = movie_ids[idx]\n",
    "    mid_map[movie_ids[idx]] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(ratings_df)\n",
    "del(user_id)\n",
    "del(movie_id)\n",
    "del(ratings)\n",
    "del(user_ids)\n",
    "del(movies_df)\n",
    "del(movie_ids)\n",
    "del(mid_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(uid_mid_pairs)):\n",
    "    uid, mid, rating = uid_mid_pairs[idx]\n",
    "    uid_mid_pairs[idx] = (uid_map[uid], mid_map[mid], rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_rating = np.mean([rating for _, _, rating in uid_mid_pairs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uids, mids, ratings = map(list, zip(*uid_mid_pairs))\n",
    "ratings_matrix = sparse.csr_matrix((ratings, (uids, mids)), shape=(len(uid_map), len(mid_map)))\n",
    "\n",
    "del(uids)\n",
    "del(mids)\n",
    "del(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, m = ratings_matrix.shape\n",
    "latent_dim = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights1 = np.full((m, m), 1.0/m)\n",
    "weights1_m, weights1_v = np.zeros((m, m)), np.zeros((m, m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights2 = np.full((m, m), 1.0/m)\n",
    "weights2_m, weights2_v = np.zeros((m, m)), np.zeros((m, m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_y = np.random.normal(0.0, 0.001, m * latent_dim).reshape((m, latent_dim))\n",
    "weights_ym, weights_yv = np.zeros((m, latent_dim)), np.zeros((m, latent_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.random.normal(0.0, 0.001, n * latent_dim).reshape((n, latent_dim))\n",
    "q = np.random.normal(0.0, 0.001, m * latent_dim).reshape((m, latent_dim))\n",
    "\n",
    "pm, qm = np.zeros((n, latent_dim)), np.zeros((m, latent_dim))\n",
    "pv, qv = np.zeros((n, latent_dim)), np.zeros((m, latent_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_u, bias_m = np.zeros(n), np.zeros(m)\n",
    "    \n",
    "b1m, b2m = np.zeros(n), np.zeros(m)\n",
    "b1v, b2v = np.zeros(n), np.zeros(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(uid_mid_pairs)\n",
    "validation_data, training_data = uid_mid_pairs[:5000], uid_mid_pairs[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(uid_mid_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ratings = np.bincount(ratings_matrix.nonzero()[0])\n",
    "normalizations = 1.0/np.sqrt(num_ratings + 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "implicit = sparse.csr_matrix((ratings_matrix != 0).astype(int).T.multiply(normalizations).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighborhood_diffs(ratings_matrix, bias_u, bias_m, mean_rating, mydata, normalizations, implicit):\n",
    "    u_idx, m_idx, _ = map(list, zip(*mydata))\n",
    "    \n",
    "    ratings = ratings_matrix[u_idx]\n",
    "    baselines = np.add.outer(bias_u[u_idx], bias_m) + mean_rating\n",
    "    \n",
    "    diff1 = sparse.csr_matrix((ratings.data - baselines[ratings.nonzero()], ratings.nonzero()), shape=baselines.shape)\n",
    "    diff1 = diff1.T.multiply(normalizations[u_idx]).T\n",
    "    \n",
    "    diff2 = implicit[u_idx]\n",
    "    \n",
    "    return diff1, diff2\n",
    "\n",
    "def get_neighborhood_scores(ratings_matrix, weights1, weights2, bias_u, bias_m, mean_rating, mydata, normalizations, implicit):\n",
    "    u_idx, m_idx, _ = map(list, zip(*mydata))\n",
    "    \n",
    "    diff1, diff2 = get_neighborhood_diffs(ratings_matrix, bias_u, bias_m, mean_rating, mydata, normalizations, implicit)\n",
    "    \n",
    "    a = diff1.multiply(weights1[m_idx]).sum(axis=1)\n",
    "    \n",
    "    b = diff1.multiply(weights2[m_idx]).sum(axis=1)\n",
    "    \n",
    "    return np.squeeze(np.asarray(a + b))\n",
    "\n",
    "def get_latent_neighborhood_scores(ratings_matrix, weights_y, mydata, normalizations, implicit):\n",
    "    u_idx, m_idx, _ = map(list, zip(*mydata))\n",
    "    \n",
    "    return implicit[u_idx].dot(weights_y)\n",
    "\n",
    "def get_ratings_errors(ratings_matrix, p, q, weights1, weights2, weights_y, bias_u, bias_m, mean_rating, mydata, normalizations, implicit):\n",
    "    u_idx, m_idx, true_ratings = map(list, zip(*mydata))\n",
    "    \n",
    "    scores = get_neighborhood_scores(ratings_matrix, weights1, weights2, bias_u, bias_m, mean_rating, mydata, normalizations, implicit)\n",
    "    latent_nscores = get_latent_neighborhood_scores(ratings_matrix, weights_y, mydata, normalizations, implicit)\n",
    "    \n",
    "    preds = np.sum((p[u_idx] + latent_nscores) * q[m_idx], axis=1) + bias_u[u_idx] + bias_m[m_idx] + mean_rating + scores\n",
    "    \n",
    "    return true_ratings - preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta, lambdas = 0.001, 0.1\n",
    "beta1, beta2 = 0.9, 0.999\n",
    "eps = 1e-8\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "num_iter, losses, last_k_losses = 0, [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    num_iter += 1\n",
    "\n",
    "    if num_iter % 1000 == 0:\n",
    "        errs_validation = get_ratings_errors(ratings_matrix, p, q, weights1, weights2, weights_y, bias_u, bias_m, mean_rating, validation_data, normalizations, implicit)\n",
    "        rmse_loss = np.sqrt(np.mean(errs_validation**2))\n",
    "\n",
    "        losses.append(rmse_loss)\n",
    "\n",
    "        print rmse_loss\n",
    "\n",
    "        if rmse_loss < 0.5:\n",
    "            break\n",
    "    \n",
    "    selected_pairs = random.sample(training_data, batch_size)\n",
    "\n",
    "    errs_train = get_ratings_errors(ratings_matrix, p, q, weights1, weights2, weights_y, bias_u, bias_m, mean_rating, selected_pairs, normalizations, implicit)\n",
    "\n",
    "    u_idx, m_idx, _ = map(list, zip(*selected_pairs))\n",
    "\n",
    "    x, y = bias_u[u_idx], bias_m[m_idx]\n",
    "    \n",
    "    u1, v1 = b1m[u_idx], b1v[u_idx]\n",
    "    u2, v2 = b2m[m_idx], b2v[m_idx]\n",
    "\n",
    "    grad1, grad2 = -(errs_train - lambdas * x), -(errs_train - lambdas * y)\n",
    "    \n",
    "    u1 = beta1 * u1 + (1 - beta1) * grad1\n",
    "    v1 = beta2 * v1 + (1 - beta2) * (grad1**2)\n",
    "    \n",
    "    x += -eta * u1/(np.sqrt(v1) + eps)\n",
    "    \n",
    "    u2 = beta1 * u2 + (1 - beta1) * grad2\n",
    "    v2 = beta2 * v2 + (1 - beta2) * (grad2**2)\n",
    "    \n",
    "    y += -eta * u2/(np.sqrt(v2) + eps)\n",
    "    \n",
    "    bias_u[u_idx], bias_m[m_idx], b1m[u_idx], b1v[u_idx], b2m[m_idx], b2v[m_idx] = x, y, u1, v1, u2, v2\n",
    "    \n",
    "    \n",
    "    diff1, diff2 = get_neighborhood_diffs(ratings_matrix, bias_u, bias_m, mean_rating, selected_pairs, normalizations, implicit)\n",
    "    \n",
    "    x, y = weights1[m_idx], weights2[m_idx]\n",
    "    \n",
    "    u1, v1 = weights1_m[m_idx], weights1_v[m_idx]\n",
    "    u2, v2 = weights2_m[m_idx], weights2_v[m_idx]\n",
    "    \n",
    "    z1, z2 = np.array(diff1.T.multiply(errs_train).T.todense()), np.array(diff2.T.multiply(errs_train).T.todense())\n",
    "    \n",
    "    grad1, grad2 = -(z1 - lambdas * x), -(z2 - lambdas * y)\n",
    "    \n",
    "    u1 = beta1 * u1 + (1 - beta1) * grad1\n",
    "    v1 = beta2 * v1 + (1 - beta2) * (grad1**2)\n",
    "    \n",
    "    x += -eta * u1/(np.sqrt(v1) + eps)\n",
    "    \n",
    "    u2 = beta1 * u2 + (1 - beta1) * grad2\n",
    "    v2 = beta2 * v2 + (1 - beta2) * (grad2**2)\n",
    "    \n",
    "    y += -eta * u2/(np.sqrt(v2) + eps)\n",
    "    \n",
    "    weights1[m_idx], weights2[m_idx], weights1_m[m_idx], weights1_v[m_idx], weights2_m[m_idx], weights2_v[m_idx] = x, y, u1, v1, u2, v2\n",
    "    \n",
    "\n",
    "    x, y = p[u_idx], q[m_idx]\n",
    "    neighborhood_scores = get_latent_neighborhood_scores(ratings_matrix, weights_y, selected_pairs, normalizations, implicit)\n",
    "    \n",
    "    u1, v1 = pm[u_idx], pv[u_idx]\n",
    "    u2, v2 = qm[m_idx], qv[m_idx]\n",
    "\n",
    "    z1, z2 = np.multiply(y.T, errs_train).T, np.multiply((x + neighborhood_scores).T, errs_train).T\n",
    "    \n",
    "    grad1, grad2 = -(z1 - lambdas * x), -(z2 - lambdas * y)\n",
    "\n",
    "    u1 = beta1 * u1 + (1 - beta1) * grad1\n",
    "    v1 = beta2 * v1 + (1 - beta2) * (grad1**2)\n",
    "\n",
    "    x += -eta * u1/(np.sqrt(v1) + eps)\n",
    "\n",
    "    u2 = beta1 * u2 + (1 - beta1) * grad2\n",
    "    v2 = beta2 * v2 + (1 - beta2) * (grad2**2)\n",
    "\n",
    "    y += -eta * u2/(np.sqrt(v2) + eps)\n",
    "    \n",
    "    p[u_idx], q[m_idx], pm[u_idx], pv[u_idx], qm[m_idx], qv[m_idx] = x, y, u1, v1, u2, v2\n",
    "    \n",
    "    \n",
    "    x = weights_y[m_idx]\n",
    "    \n",
    "    u1, v1 = weights_ym[m_idx], weights_yv[m_idx]\n",
    "    \n",
    "    latents = np.multiply(q[m_idx].T, normalizations[u_idx]).T\n",
    "    \n",
    "    z1 = np.multiply(latents.T, errs_train).T\n",
    "    grad1 = -(z1 - lambdas * x)\n",
    "    \n",
    "    u1 = beta1 * u1 + (1 - beta1) * grad1\n",
    "    v1 = beta2 * v1 + (1 - beta2) * (grad1**2)\n",
    "    \n",
    "    x += -eta * u1/(np.sqrt(v1) + eps)\n",
    "    \n",
    "    weights_y[m_idx], weights_ym[m_idx], weights_yv[m_idx] = x, u1, v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losses[:340])\n",
    "plt.xlabel(\"Number of epochs\")\n",
    "plt.ylabel(\"RMSE Loss on validation data\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
