{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import os, random\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from scipy import sparse\n",
    "\n",
    "dataset_path = \"/Users/a0m02fp/Downloads/ml-20m\"\n",
    "\n",
    "ratings_df = pd.read_csv(os.path.join(dataset_path, \"ratings.csv\"), encoding=\"utf-8\", sep=\",\", nrows=10000)\n",
    "\n",
    "user_id, movie_id, ratings = list(ratings_df[u'userId']), list(ratings_df[u'movieId']), list(ratings_df[u'rating'])\n",
    "\n",
    "uid_mid_pairs = zip(user_id, movie_id, ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid_map = dict()\n",
    "\n",
    "user_ids = sorted(list(set(user_id)))\n",
    "\n",
    "n_users = len(user_ids)\n",
    "\n",
    "for idx in range(len(user_ids)):\n",
    "    uid_map[user_ids[idx]] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = pd.read_csv(os.path.join(dataset_path, \"movies.csv\"), encoding=\"utf-8\", sep=\",\")\n",
    "\n",
    "movie_ids, mid_titles = list(movies_df[u'movieId']), list(movies_df[u'title'])\n",
    "\n",
    "n_movies = len(movie_ids)\n",
    "\n",
    "mid_to_title_map = dict()\n",
    "\n",
    "mid_map, mid_reverse_map = dict(), dict()\n",
    "\n",
    "for mid, title in zip(movie_ids, mid_titles):\n",
    "    mid_to_title_map[mid] = title\n",
    "    \n",
    "for idx in range(len(movie_ids)):\n",
    "    mid_reverse_map[idx] = movie_ids[idx]\n",
    "    mid_map[movie_ids[idx]] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(uid_mid_pairs)):\n",
    "    uid, mid, rating = uid_mid_pairs[idx]\n",
    "    uid_mid_pairs[idx] = (uid_map[uid], mid_map[mid], rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uids, mids, ratings = map(list, zip(*uid_mid_pairs))\n",
    "ratings_matrix = sparse.csr_matrix((ratings, (uids, mids)), shape=(n_users, n_movies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "uid_mat = csr_matrix(([], ([], [])), shape=(len(uid_mid_pairs), n_users))\n",
    "mid_mat = csr_matrix(([], ([], [])), shape=(len(uid_mid_pairs), n_movies))\n",
    "\n",
    "uid_mat[range(len(uid_mid_pairs)), uids] = 1\n",
    "mid_mat[range(len(uid_mid_pairs)), mids] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_df = pd.read_csv(os.path.join(dataset_path, \"tags.csv\"), encoding=\"utf-8\", sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres_df = pd.read_csv(os.path.join(dataset_path, \"movies.csv\"), encoding=\"utf-8\", sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "stdout = sys.stdout\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')\n",
    "sys.stdout = stdout\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "movie_id, tags = list(tags_df[u'movieId']), list(tags_df[u'tag'])\n",
    "\n",
    "tags = [str(tag) for tag in tags]\n",
    "\n",
    "movie_tag_map = defaultdict(list)\n",
    "\n",
    "for idx in range(len(movie_id)):\n",
    "    tag = tags[idx].lower()\n",
    "    tag = re.sub(\"[^a-zA-Z0-9 ]\", \" \", tag)\n",
    "    tag = tag.strip()\n",
    "    tag = re.sub(\"\\s+\", \" \", tag)\n",
    "    \n",
    "    if len(tag) > 0:\n",
    "        tag_words = tag.split(\" \")\n",
    "        tag = \" \".join([x for x in tag_words if x not in stop_words])\n",
    "        \n",
    "        movie_tag_map[mid_map[movie_id[idx]]].append(tag)\n",
    "            \n",
    "movie_id, genres = list(genres_df[u'movieId']), list(genres_df[u'genres'])\n",
    "\n",
    "for idx in range(len(movie_id)):\n",
    "    genre = genres[idx].lower()\n",
    "    all_genres = genre.split(\"|\")\n",
    "    \n",
    "    for gen in all_genres:\n",
    "        movie_tag_map[mid_map[movie_id[idx]]].append(gen)\n",
    "\n",
    "movie_tags = []\n",
    "\n",
    "for mid in range(n_movies):\n",
    "    movie_tags.append(\"$$$\".join(movie_tag_map[mid]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=lambda sent: sent.split(\"$$$\"), ngram_range=(1,1), stop_words='english')\n",
    "movie_tag_mat = vectorizer.fit_transform(movie_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_mat = movie_tag_mat[mids,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "implicit = sparse.csr_matrix((ratings_matrix != 0).astype(int))\n",
    "imp_mat = implicit[uids,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "mat = hstack((uid_mat, mid_mat, tag_mat, imp_mat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(mat, ratings, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVR(kernel='linear')\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "print mean_squared_error(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_0 = 0.0\n",
    "weight_0_m, weight_0_v = 0.0, 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_k = np.zeros(mat.shape[1])\n",
    "weight_k_m, weight_k_v = np.zeros(mat.shape[1]), np.zeros(mat.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 64\n",
    "factors = np.zeros((mat.shape[1], k))\n",
    "factors_m, factors_v = np.zeros((mat.shape[1], k)), np.zeros((mat.shape[1], k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_predictions(selected_data, weight_0, weight_k, factors):\n",
    "    x = selected_data.dot(factors)\n",
    "    y = selected_data.power(2).dot((factors**2))\n",
    "    \n",
    "    z = 0.5 * ((x**2) - y)\n",
    "    return weight_0 + np.squeeze(np.asarray(selected_data.multiply(weight_k).sum(axis=1))) + z.sum(axis=1)\n",
    "\n",
    "def get_errors(selected_data, weight_0, weight_k, factors, true_labels):\n",
    "    preds = get_predictions(selected_data, weight_0, weight_k, factors)\n",
    "    return true_labels - preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta, lambdas = 0.001, 0.1\n",
    "beta1, beta2 = 0.9, 0.999\n",
    "eps = 1e-8\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "num_iter, losses, last_k_losses = 0, [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    num_iter += 1\n",
    "\n",
    "    if num_iter % 10 == 0:\n",
    "        errs_validation = get_errors(X_test, weight_0, weight_k, factors, y_test)\n",
    "        rmse_loss = np.sqrt(np.mean(errs_validation**2))\n",
    "\n",
    "        losses.append(rmse_loss)\n",
    "\n",
    "        print rmse_loss\n",
    "\n",
    "        if rmse_loss < 0.5:\n",
    "            break\n",
    "    \n",
    "    selected_rows = random.sample(range(X_train.shape[0]), batch_size)\n",
    "    \n",
    "    selected_data = X_train[selected_rows,:]\n",
    "    selected_labels = np.asarray(y_train)[selected_rows]\n",
    "\n",
    "    errs_train = get_errors(selected_data, weight_0, weight_k, factors, selected_labels)\n",
    "    \n",
    "    x, u1, v1 = weight_0, weight_0_m, weight_0_v\n",
    "    \n",
    "    grad = -(np.sum(errs_train) - lambdas * x)\n",
    "    \n",
    "    u1 = beta1 * u1 + (1 - beta1) * grad\n",
    "    v1 = beta2 * v1 + (1 - beta2) * (grad**2)\n",
    "    \n",
    "    x += -eta * u1/(np.sqrt(v1) + eps)\n",
    "    \n",
    "    weight_0, weight_0_m, weight_0_v = x, u1, v1\n",
    "    \n",
    "    \n",
    "    x, u1, v1 = weight_k, weight_k_m, weight_k_v\n",
    "    \n",
    "    grad = -(selected_data.T.dot(errs_train) - lambdas * x)\n",
    "    \n",
    "    u1 = beta1 * u1 + (1 - beta1) * grad\n",
    "    v1 = beta2 * v1 + (1 - beta2) * (grad**2)\n",
    "    \n",
    "    x += -eta * u1/(np.sqrt(v1) + eps)\n",
    "    \n",
    "    weight_k, weight_k_m, weight_k_v = x, u1, v1\n",
    "    \n",
    "    \n",
    "    x, u1, v1 = factors, factors_m, factors_v\n",
    "    \n",
    "    a, b = selected_data.dot(x), selected_data.T.multiply(errs_train).T\n",
    "    c = selected_data.power(2).T.multiply(errs_train).T.tocsc()\n",
    "    \n",
    "    f = b.T.dot(a)\n",
    "    g = csr_matrix(([], ([], [])), shape=(x.shape[0], x.shape[1]))\n",
    "    \n",
    "    for k in range(batch_size):\n",
    "        g += c[k,:].T.multiply(x)\n",
    "    \n",
    "    h = f - g.toarray()\n",
    "    \n",
    "    grad = -(h - lambdas * x)\n",
    "    \n",
    "    u1 = beta1 * u1 + (1 - beta1) * grad\n",
    "    v1 = beta2 * v1 + (1 - beta2) * (grad**2)\n",
    "    \n",
    "    x += -eta * u1/(np.sqrt(v1) + eps)\n",
    "    \n",
    "    factors, factors_m, factors_v = x, u1, v1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
